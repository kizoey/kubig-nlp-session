{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "[KUBIG] 방학 NLP 세션 5주차_기다연.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgawEufyyCGd"
      },
      "source": [
        "## [5주차_기다연] 순환신경망 RNN\n",
        "1. 데이터셋 수집\n",
        "2. 태깅, 형태소 분석, 데이터 전처리\n",
        "3. word2index, index2word 정의\n",
        "3. 문장 인코딩\n",
        "4. 원핫 인코딩\n",
        "5. LSTM 이용한 챗봇 모델\n",
        "6. 예측 답변 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj2t230VyGCW"
      },
      "source": [
        "#### **과제 내용**\n",
        "5주차 순환신경망(RNN) 단원의 과제는 아래 사이트들을 참고하여 Vanilla RNN 모델로 1개의 NLP task를 구현해보는 것이다.\n",
        "<br>\n",
        "참고 사이트 :\n",
        "1. [wikidocs] https://wikidocs.net/22894\n",
        " - 스팸 메일 분류하기: 이진 분류(Binary Classification)\n",
        "\n",
        "2.  [wikidocs] https://wikidocs.net/24586, https://wikidocs.net/44249, https://wikidocs.net/94600\n",
        " - 리뷰 감성 분류하기: 감성 분류(Sentiment Analysis)\n",
        "\n",
        "3. [Kaggle] https://www.kaggle.com/roblexnana/generating-text-for-nlp-using-simplernn-with/\n",
        " - 텍스트 생성하기: 언어 모델(Language Models), 텍스트 입력시 그 다음에 나올 내용을 생성해주는 언어 모델\n",
        " - 문자(character) 단위/단어 단위/문장 단위도 가능\n",
        "\n",
        "4. [Kaggle] https://www.kaggle.com/ozkanozturk/stock-price-prediction-by-simple-rnn-and-lstm\n",
        " - 주가 예측하기: 시계열 데이터 분석 및 예측 (Tesla)\n",
        "\n",
        "5. [wikidocs] https://wikidocs.net/77246, https://diane-space.tistory.com/200\n",
        " - 챗봇 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smO8IpxXx95p"
      },
      "source": [
        "#Initial Setting (font, display)\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font', family='NanumBarunGothic') \n",
        "\n",
        "import matplotlib\n",
        "get_ipython().run_line_magic('config', \"InlineBackend.figure_format='retina'\") #화질 보정"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t81VchCe-tfF"
      },
      "source": [
        "!pip3 install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7VF-E_XyKtQ",
        "outputId": "596ed506-85a9-4c9b-e0f2-4134bb540d31"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from scipy.sparse import csc_matrix\n",
        "from scipy.sparse.linalg import svds #SVD\n",
        "from string import punctuation\n",
        "from keras import models, layers, optimizers, metrics, preprocessing \n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#NLP Preprocessing package\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "print('Ready')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vWOkd432jMc"
      },
      "source": [
        "#### 1) 데이터 수집"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61nZL-6_Rxj0",
        "outputId": "f505879f-05d7-4944-d711-e1421a476f1b"
      },
      "source": [
        "#챗봇의 질의응답 데이터 불러오기\n",
        "chatbot = pd.read_csv(\"ChatData.csv\")\n",
        "question = chatbot[\"Q\"]\n",
        "answer = chatbot[\"A\"]\n",
        "\n",
        "question = question.to_numpy()\n",
        "answer = answer.to_numpy()\n",
        "\n",
        "#데이터 양이 너무 많아 RAM 다운 에러 발생 (데이터 양 줄이기)\n",
        "question = question[:5000]\n",
        "answer = answer[:5000]\n",
        "\n",
        "print(len(question))\n",
        "print(len(answer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n",
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5j32qnK9klO",
        "outputId": "398d359f-2c50-4baa-cdf1-1f3864ac743d"
      },
      "source": [
        "for i in range(5): #5개 임의 출력\n",
        "  print(\"질문: \" + question[i])\n",
        "  print(\"답변: \" + answer[i])\n",
        "  print(\"  \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "질문: 12시 땡!\n",
            "답변: 하루가 또 가네요.\n",
            "  \n",
            "질문: 1지망 학교 떨어졌어\n",
            "답변: 위로해 드립니다.\n",
            "  \n",
            "질문: 3박4일 놀러가고 싶다\n",
            "답변: 여행은 언제나 좋죠.\n",
            "  \n",
            "질문: 3박4일 정도 놀러가고 싶다\n",
            "답변: 여행은 언제나 좋죠.\n",
            "  \n",
            "질문: PPL 심하네\n",
            "답변: 눈살이 찌푸려지죠.\n",
            "  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AODbwdlA-D1w"
      },
      "source": [
        "#### 2) Tagging 지정\n",
        "- Padding\n",
        "- Start\n",
        "- End\n",
        "- OOV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt_gzFYA9vnH"
      },
      "source": [
        "#태그할 단어\n",
        "PADDING = \"<PADDING>\"  #패딩 \n",
        "START = \"<START>\"    #시작 \n",
        "END = \"<END>\"      #끝 \n",
        "OOV = \"<OOV>\"      #out of vocabulary \n",
        "\n",
        "PADDING_INDEX = 0\n",
        "START_INDEX = 1 \n",
        "END_INDEX = 2 \n",
        "OOV_INDEX = 3 \n",
        "\n",
        "ENCODER_INPUT = 0\n",
        "DECODER_INPUT = 1\n",
        "DECODER_OUTPUT = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afcEprc7QVln"
      },
      "source": [
        "#### 3) 데이터 전처리\n",
        "- HTML 태그 제거\n",
        "- 정규표현식 이용해서 특수문자 제거\n",
        "- Okt 형태소 분석기 이용해서 형태소 추출"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kkcilNXRVDz"
      },
      "source": [
        "from konlpy.tag import Okt "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDbJ3gb2Qceg"
      },
      "source": [
        "def data_preprocess(sentences):\n",
        "    #텍스트 정제 (HTML 태그 제거)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence = BeautifulSoup(sentence, 'html.parser').text \n",
        "        sentences[i] = sentence\n",
        "\n",
        "    #텍스트 정제 (특수기호 제거)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        sentence = re.sub(r'[^ ㄱ-ㅣ가-힣]', '', sentence) #특수기호 제거, 정규 표현식\n",
        "        sentences[i] = sentence\n",
        "\n",
        "    #텍스트 정제 (형태소 추출)\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        okt = Okt()\n",
        "        clean_words = []\n",
        "        for word in okt.morphs(sentence): \n",
        "            clean_words.append(word)\n",
        "        sentence = ' '.join(clean_words)\n",
        "        sentences[i] = sentence\n",
        "\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdtJeBXLQ_3g",
        "outputId": "3da897c0-a587-4bfd-955b-eecfdb4aa7ee"
      },
      "source": [
        "question = data_preprocess(question)\n",
        "answer = data_preprocess(answer)\n",
        "print(question[:3])\n",
        "print(answer[:3])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['시 땡' '지망 학교 떨어졌어' '박일 놀러 가고 싶다']\n",
            "['하루 가 또 가네요' '위로 해 드립니다' '여행 은 언제나 좋죠']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdgF1KutRXSQ",
        "outputId": "d8725b26-83e3-4f41-b3bd-bfd62f213860"
      },
      "source": [
        "#질문+대답 1개로 합치기\n",
        "sentences = []\n",
        "sentences.extend(question)\n",
        "sentences.extend(answer)\n",
        "print(\"질문/대답 합친 문장 길이: \", len(sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "질문/대답 합친 문장 길이:  10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7Z9RXXvDywA"
      },
      "source": [
        "#### 4) 단어배열 생성\n",
        "- word2index: 단어 index화\n",
        "- index2word: index 단어화\n",
        "- 딕셔너리 구조 사용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIztdeZSEc45",
        "outputId": "5e1d44ad-026e-48d6-e263-4a36639be94a"
      },
      "source": [
        "#단어배열 생성\n",
        "words = []\n",
        "\n",
        "for sentence in sentences: #문장 단어 토큰화\n",
        "  for word in sentence.split():\n",
        "    words.append(word)\n",
        "\n",
        "\n",
        "words = [ word for word in words if len(word) > 0] #길이 0인 단어 삭제\n",
        "words = list(set(words)) #중복단어 삭제\n",
        "\n",
        "#제일 앞에 태그 단어 삽입\n",
        "words = [PADDING, START, END, OOV] + words\n",
        "print(words[:10])\n",
        "vocab_size = len(words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<PADDING>', '<START>', '<END>', '<OOV>', '치즈', '뭘', '초미세먼지', '태어나면', '단거', '모르게']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s646HfSNRjqC"
      },
      "source": [
        "#word2index, index2word 생성\n",
        "word2index = {word:index for index, word in enumerate(words)}\n",
        "index2word = {index:word for index, word in enumerate(words)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFjyF6q5EeVD"
      },
      "source": [
        "#### 5) 문장 인코딩\n",
        ": 주어진 문장에 대해서 인덱스로 변환\n",
        "- `sentences`: 입력 문장\n",
        "- `voc`: corpus, 단어사전\n",
        "- `mytype`: ENCODER_INPUT/DECODER_INPUT/DECODER_TARGET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y4C6OUKEb28"
      },
      "source": [
        "maxSequences = 30\n",
        "\n",
        "def convert_text_to_index(sentences, voc, mytype):\n",
        "  sentencesIndex = [] #sentenceIndex 모을 빈 리스트\n",
        "\n",
        "  for sentence in sentences:\n",
        "    sentenceIndex = [] #인덱스 넣을 빈 리스트\n",
        "    \n",
        "    #Decoder 입력일 경우 맨 앞에 START 태그 추가\n",
        "    if mytype == DECODER_INPUT:\n",
        "      sentenceIndex.append(voc[START])\n",
        "  \n",
        "    #문장 내 단어들 띄어쓰기로 분리\n",
        "    for word in sentence.split():\n",
        "      if voc.get(word) is not None: #단어에 해당하는 인덱스 있는 경우\n",
        "        sentenceIndex.append(voc[word]) #단어에 해당하는 인덱스 추가\n",
        "      else: #사전에 없는 경우, OOV에 추가\n",
        "        sentenceIndex.append(voc[OOV])\n",
        "\n",
        "    #최대 길이 검사\n",
        "    if mytype == DECODER_OUTPUT:\n",
        "      #Decoder 출력은 맨 마지막 END 태그 추가\n",
        "      if len(sentenceIndex) >= maxSequences:\n",
        "        sentenceIndex = sentenceIndex[:maxSequences-1] + [voc[END]]\n",
        "      else:\n",
        "        sentenceIndex += [voc[END]]\n",
        "\n",
        "    else:\n",
        "      if len(sentenceIndex) > maxSequences:\n",
        "        sentenceIndex = sentenceIndex[:maxSequences] \n",
        "\n",
        "    #최대 길이 미치지 못하는 문장의 경우, 빈 공간을 0으로 채우기(pad_sequence)\n",
        "    sentenceIndex += [word2index[PADDING]] * (maxSequences - len(sentenceIndex))\n",
        "    sentencesIndex.append(sentenceIndex)\n",
        "\n",
        "  return np.asarray(sentencesIndex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGPJR5fFv8S",
        "outputId": "efb435c9-7b4e-430d-f061-a2dfd12dfc4b"
      },
      "source": [
        "#인코더 입력, 디코더 입력, 디코더 출력 > 인덱스 변환\n",
        "\n",
        "#1. Encoder 입력 인덱스 변환\n",
        "x_encoder = convert_text_to_index(question, word2index, ENCODER_INPUT)\n",
        "print(x_encoder[0]) #12시 땡\n",
        "\n",
        "#2. Decoder 입력 인덱스 변환(START ~)\n",
        "x_decoder = convert_text_to_index(answer, word2index, DECODER_INPUT)\n",
        "print(x_decoder[0]) #START 하루 가 또 가네요\n",
        "\n",
        "#3. Decoder 목표 인덱스 변환(~END)\n",
        "y_decoder = convert_text_to_index(answer, word2index, DECODER_OUTPUT)\n",
        "print(y_decoder[0]) #하루 가 또 가네요 END"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1153 2242    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[   1 1210 3893 1114 5430    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "[1210 3893 1114 5430    2    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRD8Tx5aF9XE"
      },
      "source": [
        "#### 6) 원핫 인코딩\n",
        ": Decoder 목표를 원핫 인코딩으로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRnV8ucLF1q8"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "y_decoder = tf.keras.utils.to_categorical(y_decoder, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9DCU2rBHLZy"
      },
      "source": [
        "#### 7) LSTM 훈련 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkPCrv8vTlmz",
        "outputId": "0b8739ab-290c-46c1-831e-d9da2ecb03ce"
      },
      "source": [
        "#LSTM 모델 정의 (Encoder-Decoder 같이 있음)\n",
        "\n",
        "encoder_input = tf.keras.layers.Input(shape=(None,)) #입력 문장의 인덱스 시퀀스를 입력으로 받음\n",
        "decoder_input = tf.keras.layers.Input(shape=(None,)) #목표 문장의 인덱스 시퀀스를 입력으로 받음\n",
        "\n",
        "#Embedding 계층 (encoder_input)\n",
        "net = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
        "                                output_dim=100)(encoder_input)\n",
        "\n",
        "#state_h =hidden state, state_c =cell state\n",
        "net, state_h, state_c = tf.keras.layers.LSTM(units=128, \n",
        "                                             return_sequences=True, \n",
        "                                             return_state=True,\n",
        "                                             dropout=0.1,\n",
        "                                             recurrent_dropout=0.5)(net)\n",
        "\n",
        "#Embedding 계층 (decoder_input)\n",
        "net = tf.keras.layers.Embedding(input_dim=vocab_size, \n",
        "                                output_dim=100)(decoder_input)\n",
        "net, state_h, state_c = tf.keras.layers.LSTM(units=128, \n",
        "                                             return_sequences=True, \n",
        "                                             return_state=True,\n",
        "                                             dropout=0.1,\n",
        "                                             recurrent_dropout=0.5)(net, initial_state=[state_h, state_c]) #initial_state를 인코더의 상태로 초기화\n",
        "\n",
        "net = tf.keras.layers.Dense(units=vocab_size, \n",
        "                            activation='softmax')(net) #단어의 개수만큼 노드의 개수를 설정해 원핫 형식으로 각 단어 인덱스를 출력\n",
        "\n",
        "model = tf.keras.models.Model([encoder_input, decoder_input], net)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, None, 100)    620000      input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    620000      input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm (LSTM)                     [(None, None, 128),  117248      embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[0][0]                \n",
            "                                                                 lstm[0][1]                       \n",
            "                                                                 lstm[0][2]                       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 6200)   799800      lstm_1[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,274,296\n",
            "Trainable params: 2,274,296\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_wxiNGfXLSR"
      },
      "source": [
        "#### 8) 모델 학습 및 검증"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOeh5dKTTltR",
        "outputId": "a3e10e85-e6e1-499c-b1d7-993c17d0efa7"
      },
      "source": [
        "#인코더\n",
        "encoder_input = model.input[0] #input_1\n",
        "net = model.layers[2](encoder_input) #embedding\n",
        "net, state_h, state_c = model.layers[4](net) #lstm\n",
        "encoder_model = tf.keras.models.Model(encoder_input, \n",
        "                                      [state_h, state_c])\n",
        "\n",
        "encoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, None, 100)         620000    \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  [(None, None, 128), (None 117248    \n",
            "=================================================================\n",
            "Total params: 737,248\n",
            "Trainable params: 737,248\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eozy1MLkTlvr",
        "outputId": "62ee7967-8d53-4ea1-e04e-6c6100555f86"
      },
      "source": [
        "#디코더\n",
        "decoder_input = tf.keras.layers.Input(shape=(None,))\n",
        "state_h_input = tf.keras.layers.Input(shape=(128,))\n",
        "state_c_input = tf.keras.layers.Input(shape=(128,))   \n",
        "\n",
        "net = model.layers[-4](decoder_input) #input_3, embedding_1\n",
        "net, state_h, state_c = model.layers[-2](net, initial_state=[state_h_input, state_c_input]) #input_4, input_5\n",
        "net = model.layers[-1](net) #lstm_1\n",
        "decoder_model = tf.keras.models.Model([decoder_input, state_h_input, state_c_input], \n",
        "                                      [net, state_h, state_c])\n",
        "\n",
        "decoder_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 100)    620000      input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, None, 128),  117248      embedding_1[1][0]                \n",
            "                                                                 input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, None, 6200)   799800      lstm_1[1][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,537,048\n",
            "Trainable params: 1,537,048\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3mDooliYfPW"
      },
      "source": [
        "#인덱스를 문장으로 변환\n",
        "def convert_index_to_text(index, voc):\n",
        "    sentence = \"\"\n",
        "    for i in index: #index에 있으면 \n",
        "        if i == END_INDEX: #종료 인덱스\n",
        "            break;\n",
        "        if voc.get(i) is not None: #계속해서 문장으로 변환\n",
        "            sentence += voc[i]\n",
        "        else: #index에 없으면 OOV에 추가\n",
        "            sentence.extend(voc[OOV_INDEX])\n",
        "        sentence +=\" \"    \n",
        "    return sentence  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uif4yRh8XbRX",
        "outputId": "4dc9cbcf-a04b-4f0a-9554-d5f422d2f41a"
      },
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "             loss='categorical_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "for epoch in range(5):\n",
        "  print(\"total epoch:\", epoch+1)\n",
        "  history = model.fit([x_encoder, x_decoder], y_decoder,\n",
        "                      epochs=30,\n",
        "                      batch_size=64,\n",
        "                      verbose=0)\n",
        "  print(\"accuracy :\", history.history['accuracy'])\n",
        "  print(\"loss :\", history.history['loss'])\n",
        "    \n",
        "  #문장 예측 (3박 4일 놀러가고 싶다)\n",
        "  inputEncoder = x_encoder[2].reshape(1, x_encoder[2].shape[0]) #(30,) > (1,30)\n",
        "  inputDecoder = x_decoder[2].reshape(1, x_decoder[2].shape[0]) #(30,) > (1,30)\n",
        "  \n",
        "  results = model.predict([inputEncoder, inputDecoder])\n",
        "    \n",
        "  #결과값에 대해서 가장 큰 값의 위치를 구함\n",
        "  index = np.argmax(results[0], 1)\n",
        "  #인덱스 > 문장으로 변환\n",
        "  sentence = convert_index_to_text(index, index2word)\n",
        "  print(sentence)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total epoch: 1\n",
            "accuracy : [0.8366600275039673, 0.8393599987030029, 0.8417133092880249, 0.8437733054161072, 0.8458933234214783, 0.848026692867279, 0.8499533534049988, 0.851859986782074, 0.8542066812515259, 0.8562866449356079, 0.858513355255127, 0.8608533143997192, 0.8626400232315063, 0.8644333481788635, 0.8661400079727173, 0.8681533336639404, 0.8697800040245056, 0.8716999888420105, 0.8735399842262268, 0.8752933144569397, 0.87691330909729, 0.8785333037376404, 0.880079984664917, 0.8819266557693481, 0.8838133215904236, 0.8851733207702637, 0.8867800235748291, 0.888426661491394, 0.8895666599273682, 0.8917333483695984]\n",
            "loss : [1.1449283361434937, 1.0701204538345337, 1.0320361852645874, 1.0041048526763916, 0.9797017574310303, 0.9571412801742554, 0.9359521269798279, 0.9156511425971985, 0.8947233557701111, 0.8733197450637817, 0.8512698411941528, 0.8302785158157349, 0.8106818199157715, 0.7915635108947754, 0.7738949656486511, 0.7566031217575073, 0.7402693629264832, 0.7238367199897766, 0.7093065977096558, 0.6947138905525208, 0.6809766888618469, 0.6678271293640137, 0.6548923254013062, 0.6431360840797424, 0.6308844089508057, 0.618096113204956, 0.6076438426971436, 0.5959020256996155, 0.585299551486969, 0.5754110813140869]\n",
            "좋은 은 항상 좋죠 \n",
            "\n",
            "total epoch: 2\n",
            "accuracy : [0.8931333422660828, 0.894860029220581, 0.8964333534240723, 0.8983866572380066, 0.8999866843223572, 0.9012200236320496, 0.9029133319854736, 0.9047733545303345, 0.9057866930961609, 0.9074599742889404, 0.9085133075714111, 0.9108133316040039, 0.9119799733161926, 0.9140733480453491, 0.9148600101470947, 0.9163466691970825, 0.9174266457557678, 0.919700026512146, 0.9209200143814087, 0.9216333627700806, 0.9233666658401489, 0.9244599938392639, 0.925653338432312, 0.9265866875648499, 0.9283066391944885, 0.9289733171463013, 0.9304199814796448, 0.9316666722297668, 0.9331733584403992, 0.933573305606842]\n",
            "loss : [0.565080463886261, 0.5557525157928467, 0.5453185439109802, 0.5355657935142517, 0.5268927812576294, 0.5182839632034302, 0.5090427994728088, 0.5001242756843567, 0.4914683997631073, 0.4840332865715027, 0.4754364788532257, 0.46706053614616394, 0.45843520760536194, 0.4507153332233429, 0.4443356394767761, 0.43720224499702454, 0.42908695340156555, 0.42210376262664795, 0.4155992269515991, 0.40952688455581665, 0.4020501673221588, 0.39475545287132263, 0.3883828818798065, 0.3823320269584656, 0.37564417719841003, 0.3699140250682831, 0.3645145893096924, 0.35792505741119385, 0.35158592462539673, 0.3466480076313019]\n",
            "좋은 은 언제나 좋죠 \n",
            "\n",
            "total epoch: 3\n",
            "accuracy : [0.9353466629981995, 0.9358199834823608, 0.9364399909973145, 0.9373133182525635, 0.9386533498764038, 0.9394333362579346, 0.9404133558273315, 0.9410866498947144, 0.9418399930000305, 0.942633330821991, 0.9437866806983948, 0.9442333579063416, 0.9451533555984497, 0.9454866647720337, 0.946013331413269, 0.9471733570098877, 0.9473999738693237, 0.9486733078956604, 0.9491333365440369, 0.9502000212669373, 0.9502800107002258, 0.9515533447265625, 0.9520333409309387, 0.9523400068283081, 0.9526533484458923, 0.9533466696739197, 0.953706681728363, 0.9544933438301086, 0.9551733136177063, 0.9559199810028076]\n",
            "loss : [0.34093883633613586, 0.33614876866340637, 0.3311309218406677, 0.3253091275691986, 0.3203003704547882, 0.3156385123729706, 0.3106507658958435, 0.30638667941093445, 0.3022896647453308, 0.2971923351287842, 0.2929658591747284, 0.2882288694381714, 0.2839478850364685, 0.28022053837776184, 0.27619633078575134, 0.2706066071987152, 0.2678759694099426, 0.26275402307510376, 0.2601425051689148, 0.2558455765247345, 0.2522990107536316, 0.24690088629722595, 0.24464967846870422, 0.24135492742061615, 0.23815356194972992, 0.23471572995185852, 0.23186525702476501, 0.22740204632282257, 0.22455668449401855, 0.2208830565214157]\n",
            "여행 은 언제나 좋죠 \n",
            "\n",
            "total epoch: 4\n",
            "accuracy : [0.9562600255012512, 0.9569000005722046, 0.9570666551589966, 0.957360029220581, 0.957693338394165, 0.9583600163459778, 0.9588133096694946, 0.9592866897583008, 0.9602933526039124, 0.9604466557502747, 0.9611999988555908, 0.9610400199890137, 0.9616066813468933, 0.962286651134491, 0.9625533223152161, 0.9626266956329346, 0.9636200070381165, 0.9636333584785461, 0.9638800024986267, 0.9649400115013123, 0.9651933312416077, 0.9653333425521851, 0.9661466479301453, 0.9662066698074341, 0.9668200016021729, 0.966866672039032, 0.9673733115196228, 0.9679999947547913, 0.9681533575057983, 0.9685333371162415]\n",
            "loss : [0.2178545445203781, 0.2148243635892868, 0.21220219135284424, 0.20957785844802856, 0.20704218745231628, 0.20337709784507751, 0.20147670805454254, 0.19821184873580933, 0.19506624341011047, 0.19321607053279877, 0.19009923934936523, 0.18917015194892883, 0.18545231223106384, 0.18307101726531982, 0.18114694952964783, 0.17929136753082275, 0.17528009414672852, 0.17431867122650146, 0.1719016581773758, 0.1694476306438446, 0.16691558063030243, 0.16544513404369354, 0.16193008422851562, 0.16015926003456116, 0.15838539600372314, 0.1559097021818161, 0.1539648175239563, 0.1517062932252884, 0.14998877048492432, 0.14739109575748444]\n",
            "여행 은 언제나 좋죠 \n",
            "\n",
            "total epoch: 5\n",
            "accuracy : [0.9691533446311951, 0.969373345375061, 0.969873309135437, 0.9701600074768066, 0.9704066514968872, 0.9710933566093445, 0.9710800051689148, 0.9718199968338013, 0.9719799757003784, 0.9724466800689697, 0.9725066423416138, 0.9734533429145813, 0.9735400080680847, 0.9739866852760315, 0.9744600057601929, 0.9745200276374817, 0.9751066565513611, 0.9755200147628784, 0.9760199785232544, 0.9763000011444092, 0.9768199920654297, 0.9774133563041687, 0.9775466918945312, 0.9780666828155518, 0.9782933592796326, 0.9780133366584778, 0.9786133170127869, 0.9796800017356873, 0.9796466827392578, 0.9798933267593384]\n",
            "loss : [0.1460440307855606, 0.14436908066272736, 0.1417243331670761, 0.13984414935112, 0.13805057108402252, 0.13541629910469055, 0.13364523649215698, 0.13148252665996552, 0.13164278864860535, 0.12963101267814636, 0.12697142362594604, 0.12447576969861984, 0.1234089806675911, 0.12210750579833984, 0.12072845548391342, 0.11841624975204468, 0.11678897589445114, 0.11511333286762238, 0.11272905021905899, 0.11204217374324799, 0.10998930782079697, 0.10774759203195572, 0.10623577982187271, 0.10439283400774002, 0.1033002957701683, 0.10252724587917328, 0.1008031889796257, 0.09795393794775009, 0.09748310595750809, 0.09583985805511475]\n",
            "여행 은 언제나 좋죠 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PAZiH6gZCXl"
      },
      "source": [
        "#### 9) 문장 예측 / 예측 답변 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRUPfG8zYu-P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0064195-6f7d-4d21-f0a1-0068a2329e97"
      },
      "source": [
        "def make_predict_input(sentence):    \n",
        "    sentences=[]\n",
        "    sentences.append(sentence)\n",
        "    sentences = data_preprocess(sentences) #전처리\n",
        "    inputSeq = convert_text_to_index(sentences, #인덱스화\n",
        "                                word2index,\n",
        "                                ENCODER_INPUT)\n",
        "    return inputSeq\n",
        "    \n",
        "make_predict_input(\"3박4일 놀러가고 싶다\") #인덱스된 문장"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[4514, 4994, 3277, 6157,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEe3dFpuZao8"
      },
      "source": [
        "def generate_text(inputSeq):\n",
        "  #입력을 인코더에 넣고, 마지막 상태 구함\n",
        "  states = encoder_model.predict(inputSeq)\n",
        "        \n",
        "  #목표 시퀀스 초기화\n",
        "  targetSeq = np.zeros((1,1))\n",
        "  #<START> 시그널 추가\n",
        "  targetSeq[0,0] = START_INDEX\n",
        "  #인덱스 초기화\n",
        "  indexs=[]\n",
        "        \n",
        "  #입력되는 텍스트에 대해 디코더 반복\n",
        "  while 1:\n",
        "    decoderOuputs, stateH, stateC = decoder_model.predict([targetSeq]+states)\n",
        "    #결과를 원핫인코딩 형식으로 변환\n",
        "    index = np.argmax(decoderOuputs[0,0,:])\n",
        "    indexs.append(index)\n",
        "    #종료 체크\n",
        "    if index == END_INDEX or len(indexs) >= maxSequences:\n",
        "      break\n",
        "                \n",
        "    #targetSeq를 이전 출력으로 설정\n",
        "    targetSeq = np.zeros((1,1))\n",
        "    targetSeq[0,0] = index    #START_INDEX \n",
        "            \n",
        "    #디코더의 이전 상태를 다음 디코더 예측에 사용\n",
        "    states=[stateH, stateC]\n",
        "            \n",
        "    #인덱스를 문장으로 변환\n",
        "    sentence = convert_index_to_text(indexs, index2word)  \n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nUybVNv5aDDp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dc66b09f-3069-42a2-e740-09d4a801ec1d"
      },
      "source": [
        "inputSeq = make_predict_input(\"안녕\")\n",
        "sentence = generate_text(inputSeq)\n",
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'안녕하세요 '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_3xhZfrllbu"
      },
      "source": [
        "#### 한계점\n",
        "1. question, answer을 각각 5,000개만 사용함 (전체 데이터셋 크기:12,000이지만 다 돌리려고 하니까 계속 RAM이 폭파하는 문제 발생)\n",
        "2. epoch을 30번씩 5번밖에 돌리지 못함, default가 100번씩 10번이었음<br>\n",
        "*챗봇 성능이 아직 예상했던 것만큼 좋지 않음*"
      ]
    }
  ]
}